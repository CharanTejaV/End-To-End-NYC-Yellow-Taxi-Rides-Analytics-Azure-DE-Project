{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec0fb4d-5419-4d03-9a63-8f396aabc650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ff7d87-1a8f-4e4d-b367-26a691cab6ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting azure-storage-blob\n  Downloading azure_storage_blob-12.19.0-py3-none-any.whl (394 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 394.2/394.2 kB 4.3 MB/s eta 0:00:00\nRequirement already satisfied: typing-extensions>=4.3.0 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob) (4.3.0)\nCollecting isodate>=0.6.1\n  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 3.9 MB/s eta 0:00:00\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.10/site-packages (from azure-storage-blob) (37.0.1)\nCollecting azure-core<2.0.0,>=1.28.0\n  Downloading azure_core-1.29.5-py3-none-any.whl (192 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 192.4/192.4 kB 14.3 MB/s eta 0:00:00\nRequirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.10/site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.28.1)\nCollecting typing-extensions>=4.3.0\n  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2022.9.14)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.11)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.4)\nInstalling collected packages: typing-extensions, isodate, azure-core, azure-storage-blob\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-a9a54a27-81ff-44be-aefd-0e8b5e5c6e8f\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\nSuccessfully installed azure-core-1.29.5 azure-storage-blob-12.19.0 isodate-0.6.1 typing-extensions-4.9.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (1.4.4)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2022.1)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.10/site-packages (from pandas) (1.21.5)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install azure-storage-blob\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e27a4e-befc-4930-a953-9a39d407d1ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart the Python kernel to apply the changes\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c7339a-7be2-414a-afe0-ba2837eb84c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d53e513-783c-478f-80b7-c9f894a0c656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/databricks/driver'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4f95229-b5fd-4058-bc59-1ef08b1f19cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful for yellow_tripdata_2023-07.parquet\nConversion successful for yellow_tripdata_2023-08.parquet\nConversion successful for yellow_tripdata_2023-09.parquet\n"
     ]
    }
   ],
   "source": [
    "#Convert multiple Parquet files to CSV files\n",
    "#Connct to Azure storage account\n",
    "account_name = \"nycyellowtaxidataset\"\n",
    "account_key = \"your_account_key\"  # Replace with your actual storage account key\n",
    "container_name = \"nycyellowtaxisdatasets\"\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "blobs_list = container_client.list_blobs()\n",
    "\n",
    "# Extract blob names from the blobs list\n",
    "parquet_files = [blob.name.split('/')[1] for blob in blobs_list if blob.name.endswith(\".parquet\")]\n",
    "\n",
    "# Loop through each Parquet file\n",
    "for parquet_file in parquet_files:\n",
    "    # Download Parquet file from Azure Storage\n",
    "    parquet_blob_path = f\"parquetfiles/{parquet_file}\"\n",
    "    local_parquet_file = f\"/databricks/driver/{parquet_file}\"\n",
    "\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(parquet_blob_path)\n",
    "\n",
    "    with open(local_parquet_file, \"wb\") as data:\n",
    "        data.write(blob_client.download_blob().readall())\n",
    "\n",
    "    # Read Parquet file into a pandas DataFrame\n",
    "    parquet_df = pd.read_parquet(local_parquet_file)\n",
    "\n",
    "    # Convert DataFrame to CSV\n",
    "    csv_filename = os.path.splitext(parquet_file)[0] + \".csv\"\n",
    "    csv_blob_path = f\"csvoutputfiles/{csv_filename}\"\n",
    "    parquet_df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Upload CSV file to Azure Storage\n",
    "    blob_client = container_client.get_blob_client(csv_blob_path)\n",
    "    with open(csv_filename, \"rb\") as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "    print(f\"Conversion successful for {parquet_file}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ParquetToCsvScript",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
