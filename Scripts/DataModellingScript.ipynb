{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a00eba-17ac-451d-9334-5af62bfbf691",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import TimestampType, MapType, StringType\n",
    "from pyspark.sql.functions import monotonically_increasing_id, hour, dayofmonth, month, year, date_format, dayofweek, create_map, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e080b9e9-5d8a-41a7-9b31-64ccd473bf0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session \n",
    "spark = SparkSession.builder.appName(\"ReadBlobCSV\").getOrCreate()\n",
    "\n",
    "# Define Azure storage account parameters\n",
    "storage_account = \"nycyellowtaxidataset\"\n",
    "container = \"nycyellowtaxisdatasets\"\n",
    "access_key = \"your_acccount_key\"\n",
    "filepath = \"/FinalCSVFile/nycyellowtaxidataset.csv\"  \n",
    "\n",
    "# Connect to Azure Blob Storage\n",
    "wasbs_path = \"wasbs://%s@%s.blob.core.windows.net/%s\" % (container, storage_account, filepath)\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.%s.blob.core.windows.net\" % storage_account, access_key)\n",
    "\n",
    "# Read the CSV file from Blob storage into Spark DataFrame \n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\",\",\") \\\n",
    "    .csv(wasbs_path, header=True, inferSchema=True) \n",
    "df = df.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c55ef177-d71a-4275-9cba-4eefdae9b7d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd828c5c-51fa-4f62-98f2-301472da6890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"tpep_pickup_datetime\", \n",
    "                   df[\"tpep_pickup_datetime\"].cast(TimestampType()))\n",
    "df = df.withColumn(\"tpep_dropoff_datetime\",  \n",
    "                   df[\"tpep_dropoff_datetime\"].cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86934c44-c1ca-4932-8338-4df7aca4d65e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df= df.filter(df['RatecodeID'] != 99)\n",
    "df = df.filter((df['PULocationID'] != 264) & (df['PULocationID'] != 265))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6712fe12-32b0-4017-8c3a-07e9293fe6b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.dropDuplicates()\n",
    "# Reset index, remove existing index column\n",
    "df = df.withColumn(\"tmp_idx\", monotonically_increasing_id())\n",
    "df = df.drop(\"index\") # index column if exists\n",
    "df = df.withColumnRenamed(\"tmp_idx\", \"index\")\n",
    "# Add new trip_id column based on row number \n",
    "df = df.withColumn(\"trip_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a741aa-b5f7-4feb-bde4-2b23d13fbd61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------------+---------+--------+----------+---------+------------+\n|datetime_id|tpep_pickup_datetime|tpep_dropoff_datetime|pick_hour|pick_day|pick_month|pick_year|pick_weekday|\n+-----------+--------------------+---------------------+---------+--------+----------+---------+------------+\n|          0| 2023-08-01 00:26:44|  2023-08-01 00:45:25|        0|       1|         8|     2023|           3|\n|          1| 2023-07-01 00:29:59|  2023-07-01 00:40:15|        0|       1|         7|     2023|           7|\n|          2| 2023-08-01 00:55:42|  2023-08-01 01:00:53|        0|       1|         8|     2023|           3|\n|          3| 2023-08-01 00:32:04|  2023-08-01 01:09:03|        0|       1|         8|     2023|           3|\n|          4| 2023-08-01 00:13:37|  2023-08-01 00:41:15|        0|       1|         8|     2023|           3|\n+-----------+--------------------+---------------------+---------+--------+----------+---------+------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "datetime_dim = df.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n",
    "\n",
    "# Extract pickup attributes\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_hour\", hour(df.tpep_pickup_datetime))\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_day\", dayofmonth(df.tpep_pickup_datetime)) \n",
    "datetime_dim = datetime_dim.withColumn(\"pick_month\", month(df.tpep_pickup_datetime))\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_year\", year(df.tpep_pickup_datetime))\n",
    "datetime_dim = datetime_dim.withColumn(\"pick_weekday\", dayofweek(df.tpep_pickup_datetime))\n",
    "\n",
    "# Similarly extract drop off attributes\n",
    "# Add sequential ID column\n",
    "datetime_dim = datetime_dim.withColumn(\"datetime_id\", monotonically_increasing_id())\n",
    "\n",
    "# Select desired columns\n",
    "datetime_dim = datetime_dim.select([\"datetime_id\", \"tpep_pickup_datetime\",\"tpep_dropoff_datetime\", \n",
    "                                  \"pick_hour\",\"pick_day\",\"pick_month\",\"pick_year\",\"pick_weekday\"])\n",
    "\n",
    "#datetime_dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acb011c1-e7b8-4a23-9679-d88decf0ceef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('PULocationID', 'pickup_location_id')\n",
    "df = df.withColumnRenamed('DOLocationID', 'dropoff_location_id')\n",
    "df = df.withColumnRenamed('payment_type', 'payment_type_id')\n",
    "df = df.withColumnRenamed('RatecodeID', 'rate_code_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b845d80-145c-4e7d-8a3f-f0a384f36d48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/column.py:464: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create rate code map \n",
    "rate_code_map = create_map([\n",
    "    lit(1), lit(\"Standard rate\"), \n",
    "    lit(2), lit(\"JFK\"),\n",
    "    lit(3), lit(\"Newark\"),\n",
    "    lit(4), lit(\"Nassau or Westchester\"),\n",
    "    lit(5), lit(\"Negotiated fare\"),\n",
    "    lit(6), lit(\"Group ride\")\n",
    "])\n",
    "# Rate Code Dim Table\n",
    "rate_code_dim = df.select(\"rate_code_id\").distinct()\n",
    "rate_code_dim = rate_code_dim.withColumn(\"rate_code_name\", rate_code_map.getItem(col(\"rate_code_id\")).cast(StringType()))\n",
    "rate_code_dim = rate_code_dim.select(\"rate_code_id\",\"rate_code_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375e270b-54fd-4238-bf4d-073ac5e78c3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|payment_type_id|\n+---------------+\n|              1|\n|              3|\n|              2|\n|              4|\n|              0|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#unique_payment_types = df.select(\"payment_type_id\").distinct()\n",
    "#unique_payment_types.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c8c524-d0ad-4252-b0b7-9309250c41ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create payment type name map\n",
    "payment_map = create_map([\n",
    "    lit(0), lit(\"Credit card\"),\n",
    "    lit(1), lit(\"Cash\"),\n",
    "    lit(2), lit(\"No charge\"),\n",
    "    lit(3), lit(\"Dispute\"),\n",
    "    lit(4), lit(\"Unknown\"),\n",
    "    lit(5), lit(\"Voided trip\")\n",
    "])\n",
    "\n",
    "# Payment type dim table\n",
    "payment_type_dim = df.select(\"payment_type_id\").distinct()\n",
    "payment_type_dim = payment_type_dim.withColumn(\"payment_type\", payment_map.getItem(col(\"payment_type_id\")))\n",
    "payment_type_dim = payment_type_dim.select(\"payment_type_id\", \"payment_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894b082a-1e93-47a5-b911-477c94203704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n|payment_type_id|payment_type|\n+---------------+------------+\n|              1|        Cash|\n|              3|     Dispute|\n|              2|   No charge|\n|              4|     Unknown|\n+---------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#payment_type_dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f169c1-8fea-4c9b-a4d0-1d893d7e3153",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lookup_filepath = \"/FinalCSVFile/zone_coordinates.csv\"  \n",
    "lookup_path = \"wasbs://%s@%s.blob.core.windows.net/%s\" % (container, storage_account, lookup_filepath)\n",
    "spark.conf.set(\"fs.azure.account.key.%s.blob.core.windows.net\" % storage_account, access_key)\n",
    "zone_coordinates = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\",\",\") \\\n",
    "    .csv(lookup_path, header=True, inferSchema=True) \n",
    "zone_coordinates = zone_coordinates.coalesce(1)\n",
    "\n",
    "lookup_filepath2 = \"/FinalCSVFile/taxizone_lookup.csv\"  \n",
    "lookup_path = \"wasbs://%s@%s.blob.core.windows.net/%s\" % (container, storage_account, lookup_filepath2)\n",
    "spark.conf.set(\"fs.azure.account.key.%s.blob.core.windows.net\" % storage_account, access_key)\n",
    "taxizone_lookup = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\",\",\") \\\n",
    "    .csv(lookup_path, header=True, inferSchema=True) \n",
    "taxizone_lookup = taxizone_lookup.coalesce(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fb6c35-36b7-46d1-ad9e-155126ed9a12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pickup_location_dim = df.select(\"pickup_location_id\").distinct()\n",
    "pickup_location_dim = pickup_location_dim.select(\"pickup_location_id\") \\\n",
    "    .join(zone_coordinates.alias(\"zone\"), pickup_location_dim[\"pickup_location_id\"] == zone_coordinates[\"LocationID\"] , how=\"left_outer\") \\\n",
    "    .join(taxizone_lookup.alias(\"taxi\"), pickup_location_dim[\"pickup_location_id\"] == taxizone_lookup[\"LocationID\"] ,how=\"left_outer\")\n",
    "pickup_location_dim = pickup_location_dim.select(\"pickup_location_id\", col(\"zone.Zone\"),col(\"zone.Latitude\"),col(\"zone.Longitude\"), col(\"taxi.Borough\"))\n",
    "pickup_location_dim = pickup_location_dim.withColumnsRenamed({\"Zone\":\"pickup_zone\",\"Borough\":\"pickup_borough\", \"Latitude\":\"pickup_latitude\", \"Longitude\":\"pickup_longitude\"})\n",
    "\n",
    "dropoff_location_dim = df.select(\"dropoff_location_id\").distinct()\n",
    "dropoff_location_dim = dropoff_location_dim.select(\"dropoff_location_id\") \\\n",
    "    .join(zone_coordinates.alias(\"zone\"), df[\"dropoff_location_id\"] == zone_coordinates[\"LocationID\"] , how=\"left_outer\") \\\n",
    "    .join(taxizone_lookup.alias(\"taxi\"), dropoff_location_dim[\"dropoff_location_id\"] == taxizone_lookup[\"LocationID\"] , how=\"left_outer\")\n",
    "dropoff_location_dim = dropoff_location_dim.select(\"dropoff_location_id\", col(\"zone.Zone\"),col(\"zone.Latitude\"),col(\"zone.Longitude\"), col(\"taxi.Borough\"))\n",
    "dropoff_location_dim = dropoff_location_dim.withColumnsRenamed({\"Zone\":\"dropoff_zone\",\"Borough\":\"dropoff_borough\", \"Latitude\":\"dropoff_latitude\",\"Longitude\":\"dropoff_longitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03bbb3b4-bb9d-4a62-aed3-1d09aa2ce770",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform Spark SQL joins with aliases\n",
    "fact_table = df.join(datetime_dim, df[\"trip_id\"] == datetime_dim[\"datetime_id\"], how = \"leftouter\") \\\n",
    "    .join(rate_code_dim.alias(\"rate\"), df[\"rate_code_id\"] == rate_code_dim[\"rate_code_id\"], how = \"leftouter\") \\\n",
    "    .join(payment_type_dim.alias(\"payment\"), df[\"payment_type_id\"] == payment_type_dim[\"payment_type_id\"], how = \"leftouter\") \\\n",
    "    .join(pickup_location_dim.alias(\"pickup\"), df[\"pickup_location_id\"] == pickup_location_dim[\"pickup_location_id\"], how = \"leftouter\") \\\n",
    "    .join(dropoff_location_dim.alias(\"dropoff\"), df[\"dropoff_location_id\"] == dropoff_location_dim[\"dropoff_location_id\"], how = \"leftouter\") \\\n",
    "    .select(\n",
    "        \"trip_id\", \"VendorID\", \"datetime_id\", \"passenger_count\",\n",
    "        \"trip_distance\",\"store_and_fwd_flag\", col(\"rate.rate_code_id\"), \n",
    "        col(\"pickup.pickup_location_id\"),col(\"dropoff.dropoff_location_id\"),\n",
    "        col(\"payment.payment_type_id\"), \"fare_amount\", \"extra\", \"mta_tax\", \n",
    "        \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\", \"total_amount\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6aab50a-5221-432e-a04e-799623be0a23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fact_table = fact_table.filter(fact_table[\"VendorID\"] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385388d9-53ab-44d7-b2a1-0014dce62b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define JDBC URL\n",
    "jdbc_url = \"jdbc:sqlserver://nycyellowtaxi.database.windows.net:1433;database=nyc_yellow_taxi_data;user=yourusername;password=yourpassword;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\"\n",
    "fact_table.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"fact_table\").mode(\"overwrite\").save()\n",
    "payment_type_dim.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"payment_type_dim\").mode(\"overwrite\").save()\n",
    "pickup_location_dim.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"pickup_location_dim\").mode(\"overwrite\").save()\n",
    "dropoff_location_dim.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"dropoff_location_dim\").mode(\"overwrite\").save()\n",
    "rate_code_dim.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"rate_code_dim\").mode(\"overwrite\").save() \n",
    "datetime_dim.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"datetime_dim\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c6857e3-f128-420a-8cc8-bf058cc5baa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataModellingScript",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
